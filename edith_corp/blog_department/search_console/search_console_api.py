#!/usr/bin/env python3
"""
Search Console APIé€£æºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å®Ÿéš›ã®æ¤œç´¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æˆ¦ç•¥ã‚’å¼·åŒ–
"""

import json
import os
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from google.oauth2 import service_account
from googleapiclient.discovery import build

_THIS_DIR = Path(__file__).resolve().parent

class SearchConsoleAPI:
    """Search Console APIãƒ©ãƒƒãƒ‘ãƒ¼"""

    def __init__(self, credentials_path: str = None):
        self.credentials_path = credentials_path or os.environ.get('GOOGLE_CREDENTIALS_PATH')
        self.site_url = None
        self.service = None

        print(f"[Search Console API] åˆæœŸåŒ–é–‹å§‹")

    def authenticate(self, site_url: str):
        """APIèªè¨¼ãƒ»ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–"""

        self.site_url = site_url

        try:
            # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼
            credentials = service_account.Credentials.from_service_account_file(
                self.credentials_path,
                scopes=['https://www.googleapis.com/auth/webmasters.readonly']
            )

            # Search Console APIã‚µãƒ¼ãƒ“ã‚¹æ§‹ç¯‰
            self.service = build('searchconsole', 'v1', credentials=credentials)

            print(f"[Search Console API] âœ… èªè¨¼æˆåŠŸ: {site_url}")
            return True

        except Exception as e:
            print(f"[Search Console API] âŒ èªè¨¼å¤±æ•—: {e}")
            return False

    def get_search_analytics(self,
                            start_date: str = None,
                            end_date: str = None,
                            dimensions: List[str] = None,
                            row_limit: int = 1000) -> Dict[str, Any]:
        """æ¤œç´¢ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—"""

        if not self.service:
            print(f"[Search Console API] âš ï¸ æœªèªè¨¼ã§ã™")
            return {}

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœŸé–“ï¼šéå»28æ—¥é–“
        if not end_date:
            end_date = datetime.now().strftime('%Y-%m-%d')
        if not start_date:
            start_date = (datetime.now() - timedelta(days=28)).strftime('%Y-%m-%d')

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³
        if not dimensions:
            dimensions = ['query', 'page']

        print(f"[Search Console API] ğŸ“Š ãƒ‡ãƒ¼ã‚¿å–å¾—: {start_date} ~ {end_date}")

        try:
            request_body = {
                'startDate': start_date,
                'endDate': end_date,
                'dimensions': dimensions,
                'dimensionFilterGroups': [],
                'rowLimit': row_limit,
                'startRow': 0
            }

            response = self.service.searchanalytics().query(
                siteUrl=self.site_url,
                body=request_body
            ).execute()

            print(f"[Search Console API] âœ… {len(response.get('rows', []))}ä»¶å–å¾—")

            return self._process_search_data(response)

        except Exception as e:
            print(f"[Search Console API] âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {e}")
            return {}

    def _process_search_data(self, raw_data: Dict) -> Dict[str, Any]:
        """ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–"""

        rows = raw_data.get('rows', [])

        processed_data = {
            'summary': {
                'total_clicks': sum(row.get('clicks', 0) for row in rows),
                'total_impressions': sum(row.get('impressions', 0) for row in rows),
                'avg_ctr': 0,
                'avg_position': 0
            },
            'queries': [],
            'pages': [],
            'opportunities': []
        }

        # CTRã¨å¹³å‡æ²è¼‰é †ä½è¨ˆç®—
        if processed_data['summary']['total_impressions'] > 0:
            processed_data['summary']['avg_ctr'] = (
                processed_data['summary']['total_clicks'] /
                processed_data['summary']['total_impressions']
            )

        # ã‚¯ã‚¨ãƒªåˆ¥ãƒ‡ãƒ¼ã‚¿æ•´ç†
        query_data = {}
        page_data = {}

        for row in rows:
            keys = row.get('keys', [])

            if len(keys) >= 1:  # query
                query = keys[0]
                if query not in query_data:
                    query_data[query] = {
                        'query': query,
                        'clicks': 0,
                        'impressions': 0,
                        'ctr': 0,
                        'position': 0,
                        'position_count': 0
                    }

                query_data[query]['clicks'] += row.get('clicks', 0)
                query_data[query]['impressions'] += row.get('impressions', 0)
                query_data[query]['position'] += row.get('position', 0)
                query_data[query]['position_count'] += 1

            if len(keys) >= 2:  # page
                page = keys[1]
                if page not in page_data:
                    page_data[page] = {
                        'page': page,
                        'clicks': 0,
                        'impressions': 0,
                        'queries': []
                    }

                page_data[page]['clicks'] += row.get('clicks', 0)
                page_data[page]['impressions'] += row.get('impressions', 0)
                if query not in page_data[page]['queries']:
                    page_data[page]['queries'].append(query)

        # å¹³å‡å€¤è¨ˆç®—ã¨ã‚½ãƒ¼ãƒˆ
        for query_info in query_data.values():
            if query_info['impressions'] > 0:
                query_info['ctr'] = query_info['clicks'] / query_info['impressions']
            if query_info['position_count'] > 0:
                query_info['position'] = query_info['position'] / query_info['position_count']

        # ä¸Šä½ã‚¯ã‚¨ãƒªæŠ½å‡º
        processed_data['queries'] = sorted(
            query_data.values(),
            key=lambda x: x['impressions'],
            reverse=True
        )[:100]

        # ä¸Šä½ãƒšãƒ¼ã‚¸æŠ½å‡º
        processed_data['pages'] = sorted(
            page_data.values(),
            key=lambda x: x['clicks'],
            reverse=True
        )[:50]

        # æ”¹å–„æ©Ÿä¼šã®ç‰¹å®š
        processed_data['opportunities'] = self._identify_opportunities(query_data)

        return processed_data

    def _identify_opportunities(self, query_data: Dict[str, Dict]) -> List[Dict]:
        """æ”¹å–„æ©Ÿä¼šã®ç‰¹å®š"""

        opportunities = []

        for query, data in query_data.items():
            # ä½hanging fruitï¼šé †ä½11-20ä½ã§é«˜ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³
            if 11 <= data['position'] <= 20 and data['impressions'] > 100:
                opportunities.append({
                    'type': 'low_hanging_fruit',
                    'query': query,
                    'current_position': round(data['position'], 1),
                    'impressions': data['impressions'],
                    'potential_clicks': int(data['impressions'] * 0.1),  # 1ãƒšãƒ¼ã‚¸ç›®ã®CTRæƒ³å®š
                    'priority': 'high',
                    'action': 'æ—¢å­˜è¨˜äº‹ã®SEOæœ€é©åŒ–ã§1ãƒšãƒ¼ã‚¸ç›®ã‚’ç‹™ã†'
                })

            # é«˜ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãƒ»ä½CTR
            elif data['impressions'] > 500 and data['ctr'] < 0.02:
                opportunities.append({
                    'type': 'ctr_improvement',
                    'query': query,
                    'current_ctr': f"{data['ctr']*100:.1f}%",
                    'impressions': data['impressions'],
                    'potential_clicks': int(data['impressions'] * 0.05),  # CTRæ”¹å–„æƒ³å®š
                    'priority': 'medium',
                    'action': 'ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³æ”¹å–„'
                })

            # é †ä½ä¸‹è½æ¤œå‡ºï¼ˆå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ï¼‰
            # ã“ã“ã§ã¯ä»®å®Ÿè£…

        return sorted(opportunities, key=lambda x: x.get('potential_clicks', 0), reverse=True)[:20]

    def get_keyword_insights(self) -> Dict[str, Any]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æˆ¦ç•¥ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ"""

        # æœ€æ–°28æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
        data = self.get_search_analytics()

        if not data:
            return {}

        insights = {
            'performance_summary': data['summary'],
            'top_performing_keywords': [],
            'improvement_opportunities': data['opportunities'],
            'content_gaps': [],
            'seasonal_trends': [],
            'strategic_recommendations': []
        }

        # ãƒˆãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒŸãƒ³ã‚°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        for query_info in data['queries'][:10]:
            if query_info['clicks'] > 10:
                insights['top_performing_keywords'].append({
                    'keyword': query_info['query'],
                    'clicks': query_info['clicks'],
                    'impressions': query_info['impressions'],
                    'position': round(query_info['position'], 1),
                    'ctr': f"{query_info['ctr']*100:.1f}%"
                })

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—åˆ†æ
        insights['content_gaps'] = self._analyze_content_gaps(data['queries'])

        # æˆ¦ç•¥çš„æ¨å¥¨äº‹é …
        insights['strategic_recommendations'] = self._generate_recommendations(data, insights)

        return insights

    def _analyze_content_gaps(self, queries: List[Dict]) -> List[Dict]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—åˆ†æ"""

        gaps = []

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ã‚®ãƒ£ãƒƒãƒ—ç™ºè¦‹
        keyword_groups = {}

        for query_info in queries:
            query = query_info['query']

            # ç°¡æ˜“çš„ãªã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆå®Ÿéš›ã¯ã‚‚ã£ã¨é«˜åº¦ãªå‡¦ç†ãŒå¿…è¦ï¼‰
            if 'AI' in query or 'ChatGPT' in query:
                group = 'AIé–¢é€£'
            elif 'ãƒªãƒ¢ãƒ¼ãƒˆ' in query or 'åœ¨å®…' in query:
                group = 'ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯'
            elif 'åŠ¹ç‡' in query or 'æ”¹å–„' in query:
                group = 'æ¥­å‹™åŠ¹ç‡åŒ–'
            else:
                group = 'ãã®ä»–'

            if group not in keyword_groups:
                keyword_groups[group] = []
            keyword_groups[group].append(query_info)

        # å„ã‚°ãƒ«ãƒ¼ãƒ—ã§ã‚®ãƒ£ãƒƒãƒ—åˆ†æ
        for group, keywords in keyword_groups.items():
            total_impressions = sum(k['impressions'] for k in keywords)
            if total_impressions > 1000 and len(keywords) < 5:
                gaps.append({
                    'topic_area': group,
                    'current_keywords': len(keywords),
                    'total_impressions': total_impressions,
                    'opportunity': 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä¸è¶³',
                    'action': f'{group}é–¢é€£ã®è¨˜äº‹ã‚’å¢—ã‚„ã™'
                })

        return gaps

    def _generate_recommendations(self, data: Dict, insights: Dict) -> List[Dict]:
        """æˆ¦ç•¥çš„æ¨å¥¨äº‹é …ç”Ÿæˆ"""

        recommendations = []

        # CTRæ”¹å–„ã®æ©Ÿä¼šãŒå¤šã„å ´åˆ
        ctr_opportunities = [o for o in insights['improvement_opportunities']
                           if o['type'] == 'ctr_improvement']
        if len(ctr_opportunities) > 5:
            recommendations.append({
                'priority': 'high',
                'type': 'title_optimization',
                'description': 'å¤šãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§CTRãŒä½ã„ã€‚ã‚¿ã‚¤ãƒˆãƒ«æœ€é©åŒ–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå®Ÿæ–½æ¨å¥¨',
                'expected_impact': f'+{sum(o["potential_clicks"] for o in ctr_opportunities[:5])}ã‚¯ãƒªãƒƒã‚¯/æœˆ',
                'assigned_to': 'SEOè¶³è»½ + ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°è¶³è»½'
            })

        # ä½hanging fruitãŒå¤šã„å ´åˆ
        lhf_opportunities = [o for o in insights['improvement_opportunities']
                           if o['type'] == 'low_hanging_fruit']
        if len(lhf_opportunities) > 3:
            recommendations.append({
                'priority': 'high',
                'type': 'content_refresh',
                'description': '11-20ä½ã®è¨˜äº‹ãŒå¤šæ•°ã€‚æ—¢å­˜è¨˜äº‹ã®æœ€é©åŒ–ã§å¤§å¹…æµå…¥å¢—å¯èƒ½',
                'expected_impact': f'+{sum(o["potential_clicks"] for o in lhf_opportunities[:3])}ã‚¯ãƒªãƒƒã‚¯/æœˆ',
                'assigned_to': 'SEOè¶³è»½'
            })

        # æˆç”°æ‚ è¼”é¢¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åŠ¹æœ
        narita_keywords = [q for q in data['queries']
                         if any(word in q['query'] for word in ['è¾›è¾£', 'ç¾å®Ÿ', 'å¤±æ•—', 'æœ¬éŸ³'])]
        if narita_keywords:
            avg_ctr = sum(k['ctr'] for k in narita_keywords) / len(narita_keywords)
            recommendations.append({
                'priority': 'medium',
                'type': 'content_strategy',
                'description': f'æˆç”°é¢¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å¹³å‡CTR: {avg_ctr*100:.1f}%',
                'expected_impact': 'æ¯’èˆŒç³»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒåŠ¹æœçš„',
                'assigned_to': 'ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°è¶³è»½'
            })

        return recommendations


class SearchConsoleIntegration:
    """Search Consoleçµ±åˆç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.api = SearchConsoleAPI()
        self.site_url = None

    def setup(self, site_url: str, credentials_path: str = None):
        """Search Consoleé€£æºã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""

        self.site_url = site_url

        # èªè¨¼æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
        if credentials_path:
            self.api.credentials_path = credentials_path
        elif not self.api.credentials_path:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰
            self.api.credentials_path = str(_THIS_DIR / 'credentials' / 'claude-agent-486408-2670454f8c9f.json')

        # èªè¨¼å®Ÿè¡Œ
        success = self.api.authenticate(site_url)

        if success:
            print(f"[Search Console] âœ… é€£æºæˆåŠŸ: {site_url}")
            self._save_config()
        else:
            print(f"[Search Console] âŒ é€£æºå¤±æ•—")

        return success

    def _save_config(self):
        """è¨­å®šä¿å­˜"""

        config = {
            'site_url': self.site_url,
            'credentials_path': self.api.credentials_path,
            'setup_date': datetime.now().isoformat()
        }

        config_dir = _THIS_DIR / 'config'
        os.makedirs(config_dir, exist_ok=True)

        with open(str(config_dir / 'search_console_config.json'), 'w') as f:
            json.dump(config, f, indent=2)

        print(f"[Search Console] ğŸ“ è¨­å®šä¿å­˜å®Œäº†")

    def get_weekly_report(self) -> Dict[str, Any]:
        """é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        if not self.api.service:
            print(f"[Search Console] âš ï¸ æœªè¨­å®šã§ã™")
            return {}

        # éå»7æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')

        data = self.api.get_search_analytics(
            start_date=start_date,
            end_date=end_date,
            dimensions=['query', 'page', 'date']
        )

        insights = self.api.get_keyword_insights()

        report = {
            'period': f'{start_date} ~ {end_date}',
            'performance': data.get('summary', {}),
            'top_keywords': insights.get('top_performing_keywords', [])[:10],
            'opportunities': insights.get('improvement_opportunities', [])[:5],
            'recommendations': insights.get('strategic_recommendations', []),
            'generated_at': datetime.now().isoformat()
        }

        return report


def test_search_console_api():
    """Search Console API ãƒ†ã‚¹ãƒˆ"""

    print("ğŸ” Search Console APIé€£æºãƒ†ã‚¹ãƒˆ")
    print("=" * 60)

    # çµ±åˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
    integration = SearchConsoleIntegration()

    # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆå®Ÿéš›ã®èªè¨¼æƒ…å ±ãŒå¿…è¦ï¼‰
    site_url = 'https://example.com'  # å®Ÿéš›ã®ã‚µã‚¤ãƒˆURL
    credentials_path = 'path/to/service_account_key.json'  # å®Ÿéš›ã®èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«

    # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œç¢ºèª
    print("\n[ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰] å®Ÿéš›ã®APIæ¥ç¶šã¯ã‚¹ã‚­ãƒƒãƒ—")
    print("[ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰] ä»¥ä¸‹ã®æ©Ÿèƒ½ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ï¼š")
    print("  âœ… Search Analytics ãƒ‡ãƒ¼ã‚¿å–å¾—")
    print("  âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼šåˆ†æ")
    print("  âœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—ç™ºè¦‹")
    print("  âœ… æˆ¦ç•¥çš„æ¨å¥¨äº‹é …ç”Ÿæˆ")
    print("  âœ… é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ")

    # ãƒ¢ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
    mock_report = {
        'performance': {
            'total_clicks': 3250,
            'total_impressions': 45000,
            'avg_ctr': 0.072,
            'avg_position': 14.5
        },
        'top_keywords': [
            {'keyword': 'AIå°å…¥ å¤±æ•—', 'clicks': 245, 'position': 8.2},
            {'keyword': 'ChatGPT ä¸­å°ä¼æ¥­', 'clicks': 189, 'position': 12.5}
        ],
        'opportunities': [
            {
                'type': 'low_hanging_fruit',
                'query': 'ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯ åŠ¹ç‡åŒ–',
                'current_position': 13.5,
                'potential_clicks': 150
            }
        ]
    }

    print(f"\nğŸ“Š ãƒ¢ãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(f"  ç·ã‚¯ãƒªãƒƒã‚¯æ•°: {mock_report['performance']['total_clicks']:,}")
    print(f"  æ”¹å–„æ©Ÿä¼š: {len(mock_report['opportunities'])}ä»¶ç™ºè¦‹")
    print(f"  æœ€æœ‰æœ›ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {mock_report['opportunities'][0]['query']}")


if __name__ == "__main__":
    test_search_console_api()