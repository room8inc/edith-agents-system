#!/usr/bin/env python3
"""
SEOå°‚é–€è¶³è»½ - æ¤œç´¢æœ€é©åŒ–ã®å®ŸåƒAgent
Task Toolã¨é€£æºã—ã¦SEOæˆ¦ç•¥ã‚’è‡ªå‹•å®Ÿè¡Œ
Search Consoleå®Ÿãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæˆ¦ç•¥ç«‹æ¡ˆ
"""

import json
import re
import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

_THIS_DIR = Path(__file__).resolve().parent
_BLOG_DIR = _THIS_DIR.parent

sys.path.insert(0, str(_BLOG_DIR.parent))
from output_paths import BLOG_ARTICLES_DIR, BLOG_ARTICLES_INDEX

_ARTICLES_DIR = BLOG_ARTICLES_DIR

# Search Console APIè¿½åŠ 
sys.path.insert(0, str(_BLOG_DIR / "search_console"))
try:
    from search_console_api import SearchConsoleIntegration
except ImportError:
    SearchConsoleIntegration = None

# æˆ¦ç•¥è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ è¿½åŠ 
sys.path.insert(0, str(_BLOG_DIR.parent / "strategic_memory"))
try:
    from strategic_memory import MemoryIntegration
except ImportError:
    MemoryIntegration = None

class SEOSpecialistAshigaru:
    """SEOå°‚é–€è¶³è»½ - æ¤œç´¢æµå…¥30%å¢—åŠ ã‚’æ‹…å½“"""

    def __init__(self):
        self.rank = "è¶³è»½"
        self.specialty = "SEOæˆ¦ç•¥ãƒ»æŠ€è¡“æœ€é©åŒ–"
        self.reports_to = "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¶³è»½å¤§å°†"
        self.kpi_target = "æ¤œç´¢æµå…¥30%å¢—åŠ "

        # Search Consoleé€£æº
        self.search_console = None
        if SearchConsoleIntegration:
            self.search_console = SearchConsoleIntegration()
            print(f"[SEOè¶³è»½] Search Consoleé€£æºæº–å‚™å®Œäº†")

        # æˆ¦ç•¥è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ é€£æº
        self.memory_integration = None
        if MemoryIntegration:
            self.memory_integration = MemoryIntegration()
            print(f"[SEOè¶³è»½] æˆ¦ç•¥è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ é€£æºå®Œäº†")

        print(f"[SEOè¶³è»½] é…å±å®Œäº† - {self.kpi_target}ã‚’ç›®æ¨™ã«ç¨¼åƒé–‹å§‹")

    def analyze_keyword_opportunities(self, article_topic: str) -> Dict[str, Any]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼šåˆ†æï¼ˆSearch Consoleå®Ÿãƒ‡ãƒ¼ã‚¿åˆ©ç”¨ï¼‰"""

        print(f"[SEOè¶³è»½] ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æé–‹å§‹: {article_topic}")

        # Search Consoleã‹ã‚‰å®Ÿãƒ‡ãƒ¼ã‚¿å–å¾—
        real_search_data = None
        if self.search_console and self.search_console.api.service:
            print(f"[SEOè¶³è»½] Search Consoleå®Ÿãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
            real_search_data = self.search_console.api.get_keyword_insights()

        # å®Ÿãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯æ´»ç”¨ã€ãªã‘ã‚Œã°ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿
        if real_search_data:
            keyword_analysis = self._analyze_with_real_data(article_topic, real_search_data)
        else:
            # å¾“æ¥ã®ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            keyword_analysis = {
            "primary_keywords": [
                {"keyword": "AIå°å…¥ å¤±æ•—", "volume": "é«˜", "difficulty": "ä¸­", "intent": "å•é¡Œè§£æ±º"},
                {"keyword": "ChatGPT å°å…¥ ä¸­å°ä¼æ¥­", "volume": "ä¸­", "difficulty": "ä½", "intent": "æƒ…å ±åé›†"}
            ],
            "secondary_keywords": [
                {"keyword": "AIæ´»ç”¨ ç¾å®Ÿ", "volume": "ä¸­", "difficulty": "ä½"},
                {"keyword": "æ¥­å‹™åŠ¹ç‡åŒ– AI", "volume": "é«˜", "difficulty": "é«˜"}
            ],
            "content_gap_opportunities": [
                "å¤±æ•—äº‹ä¾‹ã®å…·ä½“çš„åˆ†æãŒç«¶åˆã«ä¸è¶³",
                "ROIè¨ˆç®—ã®å®Ÿè·µä¾‹ãŒå°‘ãªã„",
                "ä¸­å°ä¼æ¥­å‘ã‘ã®æ®µéšçš„å°å…¥æ‰‹é †ãŒä¸ååˆ†"
            ],
            "search_intent": "å•é¡Œè§£æ±ºå‹ï¼ˆãƒã‚¦ãƒ„ãƒ¼é‡è¦–ï¼‰",
            "recommended_structure": {
                "h1": "å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ˜ç¢ºãªæç¤º",
                "h2": ["åŸå› åˆ†æ", "æ”¹å–„æ‰‹é †", "æˆåŠŸäº‹ä¾‹"],
                "h3": ["å…·ä½“ä¾‹", "ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ", "ROIè©¦ç®—"]
            }
        }

        print(f"[SEOè¶³è»½] âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æå®Œäº†")
        print(f"[SEOè¶³è»½] ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {len(keyword_analysis['primary_keywords'])}å€‹")
        print(f"[SEOè¶³è»½] ç«¶åˆã‚®ãƒ£ãƒƒãƒ—: {len(keyword_analysis['content_gap_opportunities'])}å€‹ç™ºè¦‹")

        return keyword_analysis

    def _analyze_with_real_data(self, topic: str, search_data: Dict) -> Dict[str, Any]:
        """Search Consoleå®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸåˆ†æ"""

        print(f"[SEOè¶³è»½] ğŸ¯ å®Ÿãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹")

        # å®Ÿéš›ã®æ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‹ã‚‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        top_keywords = search_data.get('top_performing_keywords', [])
        opportunities = search_data.get('improvement_opportunities', [])

        # ãƒˆãƒ”ãƒƒã‚¯ã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        related_keywords = []
        for kw in top_keywords:
            keyword_text = kw.get('keyword', '')
            if any(term in keyword_text.lower() for term in topic.lower().split()):
                related_keywords.append({
                    "keyword": keyword_text,
                    "clicks": kw.get('clicks', 0),
                    "position": kw.get('position', 0),
                    "ctr": kw.get('ctr', '0%'),
                    "volume": "å®Ÿæ¸¬å€¤ã‚ã‚Š",
                    "difficulty": self._estimate_difficulty(kw.get('position', 0))
                })

        # æ”¹å–„æ©Ÿä¼šã‹ã‚‰æ–°è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å€™è£œ
        new_keyword_candidates = []
        for opp in opportunities[:5]:
            if opp.get('type') == 'low_hanging_fruit':
                new_keyword_candidates.append({
                    "keyword": opp.get('query', ''),
                    "current_position": opp.get('current_position', 0),
                    "potential_clicks": opp.get('potential_clicks', 0),
                    "priority": opp.get('priority', 'medium'),
                    "action": opp.get('action', '')
                })

        keyword_analysis = {
            "data_source": "Search Consoleå®Ÿãƒ‡ãƒ¼ã‚¿",
            "analysis_date": datetime.now().isoformat(),
            "primary_keywords": related_keywords[:5] if related_keywords else self._get_default_keywords(topic),
            "improvement_opportunities": new_keyword_candidates,
            "content_gap_opportunities": search_data.get('content_gaps', []),
            "search_intent": self._determine_search_intent(related_keywords),
            "recommended_structure": self._create_structure_recommendation(search_data),
            "real_data_insights": {
                "total_impressions": search_data.get('performance_summary', {}).get('total_impressions', 0),
                "avg_position": search_data.get('performance_summary', {}).get('avg_position', 0),
                "top_queries_count": len(top_keywords)
            }
        }

        print(f"[SEOè¶³è»½] âœ… å®Ÿãƒ‡ãƒ¼ã‚¿åˆ†æå®Œäº†")
        print(f"[SEOè¶³è»½] é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {len(related_keywords)}å€‹")
        print(f"[SEOè¶³è»½] æ”¹å–„æ©Ÿä¼š: {len(new_keyword_candidates)}å€‹")

        # é‡è¦ãªç™ºè¦‹ã‚’è‡ªå‹•ä¿å­˜
        if self.memory_integration:
            # é«˜CTRã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è‡ªå‹•è¨˜éŒ²
            for kw in related_keywords:
                if float(kw.get('ctr', '0%').replace('%', '')) > 20:
                    self.memory_integration.memory.auto_save_insight(
                        "keyword_discovery",
                        {
                            "keyword": kw["keyword"],
                            "ctr": kw["ctr"],
                            "position": kw["position"],
                            "clicks": kw["clicks"]
                        },
                        f"SEOåˆ†æã§é«˜CTRã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç™ºè¦‹: {kw['keyword']}"
                    )

            # æ”¹å–„æ©Ÿä¼šã®è‡ªå‹•è¨˜éŒ²
            if new_keyword_candidates:
                self.memory_integration.memory.auto_save_insight(
                    "success_pattern",
                    {
                        "type": "seo_opportunities",
                        "description": f"{len(new_keyword_candidates)}å€‹ã®æ”¹å–„æ©Ÿä¼šç™ºè¦‹",
                        "metrics": {
                            "total_potential_clicks": sum(k.get('potential_clicks', 0) for k in new_keyword_candidates)
                        },
                        "opportunities": new_keyword_candidates[:3]  # ä¸Šä½3ã¤
                    },
                    "Search Consoleåˆ†æã«ã‚ˆã‚‹æ”¹å–„æ©Ÿä¼š"
                )

        return keyword_analysis

    def _estimate_difficulty(self, position: float) -> str:
        """é †ä½ã‹ã‚‰é›£æ˜“åº¦ã‚’æ¨å®š"""
        if position <= 10:
            return "é«˜"
        elif position <= 20:
            return "ä¸­"
        else:
            return "ä½"

    def _determine_search_intent(self, keywords: List[Dict]) -> str:
        """æ¤œç´¢æ„å›³ã®åˆ¤å®š"""
        if not keywords:
            return "æƒ…å ±åé›†å‹"

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æ„å›³ã‚’æ¨å®š
        problem_keywords = ['å¤±æ•—', 'å•é¡Œ', 'èª²é¡Œ', 'æ”¹å–„']
        how_keywords = ['æ–¹æ³•', 'ã‚„ã‚Šæ–¹', 'ã‚¬ã‚¤ãƒ‰', 'æ‰‹é †']

        problem_count = sum(1 for kw in keywords
                          if any(term in kw.get('keyword', '') for term in problem_keywords))
        how_count = sum(1 for kw in keywords
                       if any(term in kw.get('keyword', '') for term in how_keywords))

        if problem_count > how_count:
            return "å•é¡Œè§£æ±ºå‹"
        elif how_count > 0:
            return "å®Ÿè·µã‚¬ã‚¤ãƒ‰å‹"
        else:
            return "æƒ…å ±åé›†å‹"

    def _create_structure_recommendation(self, search_data: Dict) -> Dict:
        """Search Dataã«åŸºã¥ãæ§‹é€ æ¨å¥¨"""
        return {
            "h1": "æ¤œç´¢æ„å›³ã«åˆã‚ã›ãŸæ˜ç¢ºãªèª²é¡Œæç¤º",
            "h2": ["ç¾çŠ¶åˆ†æ", "è§£æ±ºç­–", "å®Ÿè·µæ‰‹é †", "åŠ¹æœæ¸¬å®š"],
            "h3": ["å…·ä½“ä¾‹", "ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ", "æ•°å€¤ãƒ‡ãƒ¼ã‚¿"],
            "optimization_notes": "Search Consoleãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæœ€é©åŒ–å®Ÿæ–½"
        }

    def _get_default_keywords(self, topic: str) -> List[Dict]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼‰"""
        return [
            {"keyword": f"{topic} èª²é¡Œ", "volume": "æ¨å®š", "difficulty": "ä¸­"},
            {"keyword": f"{topic} è§£æ±ºç­–", "volume": "æ¨å®š", "difficulty": "ä¸­"}
        ]

    # â”€â”€ ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è§£æ â”€â”€

    def _parse_markdown(self, raw_content: str) -> Dict[str, Any]:
        """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’è§£æã—ã¦æ§‹é€ ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™"""
        lines = raw_content.split('\n')
        title = ""
        headings = []
        paragraphs = []
        current_para = []

        for line in lines:
            stripped = line.strip()
            # H1 ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆæœ€åˆã®1ã¤ã ã‘ï¼‰
            if stripped.startswith('# ') and not stripped.startswith('## ') and not title:
                title = stripped[2:].strip().strip('*')
            # H2
            elif stripped.startswith('## '):
                if current_para:
                    paragraphs.append(' '.join(current_para))
                    current_para = []
                headings.append({"level": "h2", "text": stripped.lstrip('#').strip()})
            # H3
            elif stripped.startswith('### '):
                if current_para:
                    paragraphs.append(' '.join(current_para))
                    current_para = []
                headings.append({"level": "h3", "text": stripped.lstrip('#').strip()})
            # é€šå¸¸ãƒ†ã‚­ã‚¹ãƒˆè¡Œï¼ˆç©ºè¡Œãƒ»åŒºåˆ‡ã‚Šãƒ»ç”»åƒã‚’é™¤ãï¼‰
            elif stripped and not stripped.startswith('---') and not stripped.startswith('!['):
                # Bold / italic ç­‰ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è¨˜æ³•ã‚’é™¤å»ã—ã¦ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«
                plain = re.sub(r'\*{1,2}(.+?)\*{1,2}', r'\1', stripped)
                current_para.append(plain)
            elif not stripped and current_para:
                paragraphs.append(' '.join(current_para))
                current_para = []

        if current_para:
            paragraphs.append(' '.join(current_para))

        return {
            "title": title,
            "headings": headings,
            "paragraphs": paragraphs,
            "first_paragraph": paragraphs[0] if paragraphs else "",
            "char_count": len(raw_content),
        }

    def _analyze_keyword_presence(self, raw_content: str, keyword_data: Dict) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾çŠ¶æ³ã‚’åˆ†æ"""
        content_lower = raw_content.lower()
        results = []

        for kw in keyword_data.get("primary_keywords", []):
            keyword = kw.get("keyword", "")
            if not keyword:
                continue
            count = content_lower.count(keyword.lower())
            density = round(count * len(keyword) / max(len(raw_content), 1) * 100, 2)
            results.append({
                "keyword": keyword,
                "count": count,
                "density_pct": density,
                "present": count > 0,
            })

        return {
            "keyword_occurrences": results,
            "missing_keywords": [r["keyword"] for r in results if not r["present"]],
        }

    # â”€â”€ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹é€ æœ€é©åŒ– â”€â”€

    def optimize_content_structure(self, raw_content: str, keyword_data: Dict) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹é€ ã®æœ€é©åŒ– â€” å®Ÿã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è§£æã—ã¦æ”¹å–„ææ¡ˆã‚’è¿”ã™"""

        print(f"[SEOè¶³è»½] ã‚³ãƒ³ãƒ†ãƒ³ãƒ„SEOæœ€é©åŒ–é–‹å§‹...")

        parsed = self._parse_markdown(raw_content)
        keyword_stats = self._analyze_keyword_presence(raw_content, keyword_data)

        optimized_content = {
            "title": self._optimize_title(parsed, keyword_data),
            "meta_description": self._generate_meta_description(parsed, keyword_data),
            "heading_structure": self._optimize_headings(parsed, keyword_data),
            "internal_links": self._suggest_internal_links(parsed),
            "featured_snippet_optimization": self._optimize_for_snippets(parsed, keyword_data),
            "content_stats": {
                "char_count": parsed["char_count"],
                "heading_count": len(parsed["headings"]),
                "h2_count": sum(1 for h in parsed["headings"] if h["level"] == "h2"),
                "h3_count": sum(1 for h in parsed["headings"] if h["level"] == "h3"),
                "paragraph_count": len(parsed["paragraphs"]),
            },
            "keyword_presence": keyword_stats,
        }

        print(f"[SEOè¶³è»½] âœ… SEOæœ€é©åŒ–å®Œäº†")
        print(f"[SEOè¶³è»½] æ–‡å­—æ•°: {parsed['char_count']}å­— / è¦‹å‡ºã—: {len(parsed['headings'])}å€‹")
        print(f"[SEOè¶³è»½] æœ€é©åŒ–è¦ç´ : {len(optimized_content)}é …ç›®")

        return optimized_content

    def _optimize_title(self, parsed: Dict, keyword_data: Dict) -> Dict[str, Any]:
        """å®Ÿã‚¿ã‚¤ãƒˆãƒ«ã‚’åˆ†æã—ã¦æœ€é©åŒ–ææ¡ˆ"""
        actual_title = parsed.get("title", "")
        title_len = len(actual_title)

        # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹
        primary_keywords = keyword_data.get("primary_keywords", [])
        keywords_in_title = []
        keywords_missing = []
        for kw in primary_keywords:
            keyword = kw.get("keyword", "")
            if keyword and keyword.lower() in actual_title.lower():
                keywords_in_title.append(keyword)
            elif keyword:
                keywords_missing.append(keyword)

        issues = []
        if title_len > 60:
            issues.append(f"ã‚¿ã‚¤ãƒˆãƒ«ãŒ{title_len}æ–‡å­—ã§é•·ã™ãã‚‹ï¼ˆæ¨å¥¨: 30-60æ–‡å­—ï¼‰ã€‚æ¤œç´¢çµæœã§åˆ‡ã‚Œã‚‹å¯èƒ½æ€§ã‚ã‚Š")
        elif title_len < 15:
            issues.append(f"ã‚¿ã‚¤ãƒˆãƒ«ãŒ{title_len}æ–‡å­—ã§çŸ­ã™ãã‚‹ï¼ˆæ¨å¥¨: 30-60æ–‡å­—ï¼‰")
        if keywords_missing and primary_keywords:
            issues.append(f"ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æœªå«æœ‰: {', '.join(keywords_missing[:3])}")

        return {
            "actual_title": actual_title,
            "title_length": title_len,
            "keywords_included": keywords_in_title,
            "keywords_missing": keywords_missing,
            "issues": issues,
            "score": "good" if not issues else "needs_improvement",
        }

    def _generate_meta_description(self, parsed: Dict, keyword_data: Dict) -> Dict[str, Any]:
        """å®Ÿã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³å€™è£œã‚’ç”Ÿæˆ"""
        first_para = parsed.get("first_paragraph", "")
        title = parsed.get("title", "")

        # æœ€åˆã®æ®µè½ã‚’120æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚
        if len(first_para) > 120:
            desc_base = first_para[:117] + "..."
        else:
            desc_base = first_para

        # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®1ã¤ç›®ã‚’æŠ½å‡º
        primary_kw = ""
        for kw in keyword_data.get("primary_keywords", []):
            if kw.get("keyword"):
                primary_kw = kw["keyword"]
                break

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒdescã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        kw_in_desc = primary_kw.lower() in desc_base.lower() if primary_kw else True

        return {
            "suggested_description": desc_base,
            "description_length": len(desc_base),
            "primary_keyword_included": kw_in_desc,
            "recommendation": "ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã¯120-160æ–‡å­—ãŒæœ€é©ã€‚ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‡ªç„¶ã«å«ã‚ã‚‹ã€‚",
        }

    def _optimize_headings(self, parsed: Dict, keyword_data: Dict) -> Dict[str, Any]:
        """å®Ÿè¦‹å‡ºã—æ§‹é€ ã‚’åˆ†æã—ã¦SEOææ¡ˆ"""
        headings = parsed.get("headings", [])

        primary_keywords = [kw.get("keyword", "").lower()
                            for kw in keyword_data.get("primary_keywords", []) if kw.get("keyword")]

        analyzed = []
        for h in headings:
            text_lower = h["text"].lower()
            kw_match = [kw for kw in primary_keywords if kw in text_lower]
            analyzed.append({
                "level": h["level"],
                "text": h["text"],
                "keywords_found": kw_match,
                "has_keyword": len(kw_match) > 0,
            })

        h2_count = sum(1 for h in headings if h["level"] == "h2")
        h3_count = sum(1 for h in headings if h["level"] == "h3")
        headings_with_kw = sum(1 for a in analyzed if a["has_keyword"])

        issues = []
        if h2_count < 2:
            issues.append("H2ãŒå°‘ãªã„ï¼ˆæ¨å¥¨: 3-6å€‹ï¼‰ã€‚å†…å®¹ã‚’é©åˆ‡ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†ã‘ã™ã‚‹ã¨SEOåŠ¹æœå‘ä¸Š")
        if h2_count > 0 and headings_with_kw == 0:
            issues.append("ã©ã®è¦‹å‡ºã—ã«ã‚‚ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ãªã„ã€‚H2ã«è‡ªç„¶ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚ã‚‹ã¨è‰¯ã„")

        return {
            "actual_headings": analyzed,
            "h2_count": h2_count,
            "h3_count": h3_count,
            "headings_with_keyword": headings_with_kw,
            "issues": issues,
        }

    def _load_articles_index(self) -> List[Dict]:
        """articles_index.json ã‹ã‚‰å…¨è¨˜äº‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        if BLOG_ARTICLES_INDEX.exists():
            try:
                data = json.loads(BLOG_ARTICLES_INDEX.read_text(encoding="utf-8"))
                return data.get("articles", [])
            except (json.JSONDecodeError, OSError):
                pass

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¹ã‚­ãƒ£ãƒ³ï¼ˆEDITHç”Ÿæˆè¨˜äº‹ã®ã¿ï¼‰
        results = []
        if _ARTICLES_DIR.exists():
            for meta_file in sorted(_ARTICLES_DIR.glob("*/meta.json")):
                try:
                    meta = json.loads(meta_file.read_text(encoding="utf-8"))
                    results.append(meta)
                except (json.JSONDecodeError, OSError):
                    continue
        return results

    def _suggest_internal_links(self, parsed: Dict) -> List[Dict]:
        """å…¨è¨˜äº‹ï¼ˆéå»è¨˜äº‹å«ã‚€ï¼‰ã‹ã‚‰é–¢é€£ã™ã‚‹å†…éƒ¨ãƒªãƒ³ã‚¯ã‚’ææ¡ˆ"""
        suggestions = []

        all_articles = self._load_articles_index()
        if not all_articles:
            return suggestions

        article_title = parsed.get("title", "").lower()
        article_text = " ".join(parsed.get("paragraphs", [])).lower()

        for entry in all_articles:
            other_title = entry.get("title", "")
            other_slug = entry.get("slug", "")

            # åŒã˜è¨˜äº‹ã¯ã‚¹ã‚­ãƒƒãƒ—
            if other_title.lower() == article_title:
                continue

            # ã‚¿ã‚¤ãƒˆãƒ«ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            title_words = [w for w in other_title.lower().split() if len(w) >= 2]
            match_score = sum(1 for w in title_words if w in article_text)

            # ã‚¿ã‚°ãŒã‚ã‚Œã°ã‚¿ã‚°ãƒãƒƒãƒã‚‚åŠ ç®—
            other_tags = [t.lower() for t in entry.get("tags", [])]
            tag_matches = [t for t in other_tags if t in article_text]
            match_score += len(tag_matches)

            if match_score > 0:
                suggestions.append({
                    "anchor_text": other_title,
                    "target_slug": other_slug,
                    "url": entry.get("url", f"https://www.room8.co.jp/{other_slug}/"),
                    "relevance_score": match_score,
                    "source": entry.get("source", "wordpress"),
                })

        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½5ä»¶
        suggestions.sort(key=lambda x: x["relevance_score"], reverse=True)
        return suggestions[:5]

    def _optimize_for_snippets(self, parsed: Dict, keyword_data: Dict) -> Dict[str, Any]:
        """å®Ÿã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰å¼·èª¿ã‚¹ãƒ‹ãƒšãƒƒãƒˆå€™è£œã‚’æŠ½å‡º"""
        paragraphs = parsed.get("paragraphs", [])
        headings = parsed.get("headings", [])

        # ç–‘å•æ–‡ã®è¦‹å‡ºã—ã‚’æ¢ã™ï¼ˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã®Q&Aå€™è£œï¼‰
        qa_candidates = []
        for i, h in enumerate(headings):
            if 'ï¼Ÿ' in h["text"] or '?' in h["text"]:
                # æ¬¡ã®æ®µè½ã‚’å›ç­”å€™è£œã¨ã™ã‚‹
                # è¦‹å‡ºã—ä½ç½®ã¯æ¦‚ç®—ï¼ˆæ®µè½ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨1:1ã§ã¯ãªã„ãŒè¿‘ä¼¼ï¼‰
                qa_candidates.append({
                    "question": h["text"],
                    "heading_level": h["level"],
                })

        # ãƒªã‚¹ãƒˆå½¢å¼ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¢ã™ï¼ˆç®‡æ¡æ›¸ããƒ»ç•ªå·ä»˜ãï¼‰
        list_items = []
        for p in paragraphs:
            lines = p.split(' ')
            for line in lines:
                if re.match(r'^[\dï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼]+[\.\)ï¼‰]', line.strip()):
                    list_items.append(line.strip())
                elif line.strip().startswith('- ') or line.strip().startswith('ãƒ»'):
                    list_items.append(line.strip())

        return {
            "qa_candidates": qa_candidates[:5],
            "list_content_found": len(list_items) > 0,
            "list_items_count": len(list_items),
            "recommendation": "ç–‘å•æ–‡H2ã®ç›´å¾Œã«ç°¡æ½”ãªå›ç­”ï¼ˆ2-3æ–‡ï¼‰ã‚’é…ç½®ã™ã‚‹ã¨å¼·èª¿ã‚¹ãƒ‹ãƒšãƒƒãƒˆç²å¾—ç‡ãŒä¸ŠãŒã‚‹"
                              if qa_candidates else
                              "H2ã«ç–‘å•æ–‡ï¼ˆã€‡ã€‡ã¨ã¯ï¼Ÿç­‰ï¼‰ã‚’1-2å€‹å…¥ã‚Œã‚‹ã¨å¼·èª¿ã‚¹ãƒ‹ãƒšãƒƒãƒˆå€™è£œã«ãªã‚‹",
        }

    def execute_seo_optimization(self, article_data: Dict) -> Dict[str, Any]:
        """SEOæœ€é©åŒ–ã®å®Œå…¨å®Ÿè¡Œ"""

        print(f"\n[SEOè¶³è»½] ğŸ“Š SEOæœ€é©åŒ–ã‚¿ã‚¹ã‚¯é–‹å§‹")
        print(f"[SEOè¶³è»½] å¯¾è±¡è¨˜äº‹: {article_data.get('topic', 'untitled')}")

        # 1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
        keyword_analysis = self.analyze_keyword_opportunities(article_data.get('topic', ''))

        # 2. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹é€ æœ€é©åŒ–
        seo_optimization = self.optimize_content_structure(
            article_data.get('content', ''),
            keyword_analysis
        )

        # 3. æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        seo_report = {
            "keyword_analysis": keyword_analysis,
            "seo_optimization": seo_optimization,
            "expected_impact": {
                "search_traffic_increase": "30%",
                "ranking_improvement": "å¹³å‡5ä½å‘ä¸ŠæœŸå¾…",
                "click_through_rate": "12%å‘ä¸ŠæœŸå¾…"
            },
            "implementation_status": "å®Œäº†",
            "next_monitoring": "2é€±é–“å¾Œã®é †ä½ãƒã‚§ãƒƒã‚¯",
            "optimized_at": datetime.now().isoformat()
        }

        print(f"[SEOè¶³è»½] âœ… SEOæœ€é©åŒ–å®Œäº†")
        print(f"[SEOè¶³è»½] æœŸå¾…åŠ¹æœ: æ¤œç´¢æµå…¥30%å¢—åŠ ")

        return seo_report

def test_seo_agent():
    """SEOè¶³è»½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    seo_agent = SEOSpecialistAshigaru()

    test_article = {
        "topic": "ChatGPTå°å…¥ã§å¤±æ•—ã™ã‚‹ä¸­å°ä¼æ¥­ã®ç‰¹å¾´",
        "content": "ã‚µãƒ³ãƒ—ãƒ«è¨˜äº‹å†…å®¹..."
    }

    result = seo_agent.execute_seo_optimization(test_article)

    print(f"\nğŸ¯ SEOæœ€é©åŒ–çµæœ:")
    print(f"  ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(result['keyword_analysis']['primary_keywords'])}")
    print(f"  å†…éƒ¨ãƒªãƒ³ã‚¯ææ¡ˆ: {len(result['seo_optimization']['internal_links'])}")
    print(f"  æœŸå¾…åŠ¹æœ: {result['expected_impact']['search_traffic_increase']} æµå…¥å¢—åŠ ")

if __name__ == "__main__":
    test_seo_agent()